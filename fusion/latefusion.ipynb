{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "from torch_geometric.data import Data\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class HypergraphNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.5):\n",
    "        super(HypergraphNeuralNetwork, self).__init__()\n",
    "\n",
    "        # Hypergraph convolution layers\n",
    "        self.hgconv1 = HypergraphConv(in_dim, hidden_dim)\n",
    "        self.hgconv2 = HypergraphConv(hidden_dim, out_dim)\n",
    "\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.hgconv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.hgconv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "# Define HGNN model\n",
    "in_dim = 2048  # 2048 (ResNet50 features)\n",
    "hidden_dim = 1024  # Increased hidden dimension\n",
    "out_dim = 2  # Number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/6x2ghpdd4dzc3df6gt3zdn2c0000gn/T/ipykernel_63217/2397375762.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_model.load_state_dict(torch.load(\"model_weights2.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HypergraphNeuralNetwork(\n",
       "  (hgconv1): HypergraphConv(2048, 1024)\n",
       "  (hgconv2): HypergraphConv(1024, 2)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate the model structure\n",
    "img_model = HypergraphNeuralNetwork(in_dim, hidden_dim, out_dim)\n",
    "\n",
    "# Load the saved state_dict\n",
    "img_model.load_state_dict(torch.load(\"model_weights2.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "img_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dhg.models import HGNN\n",
    "\n",
    "class MyHGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.hgnn = HGNN(in_dim, hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, hg):\n",
    "        return self.hgnn(x, hg)\n",
    "\n",
    "in_dim = 100     # embedding dimension (100 for FastText)\n",
    "hidden_dim = 64\n",
    "out_dim = 2      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/6x2ghpdd4dzc3df6gt3zdn2c0000gn/T/ipykernel_63217/1068402849.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_model.load_state_dict(torch.load(\"model_weights.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyHGNN(\n",
       "  (hgnn): HGNN(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.5, inplace=False)\n",
       "        (theta): Linear(in_features=100, out_features=64, bias=True)\n",
       "      )\n",
       "      (1): HGNNConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.5, inplace=False)\n",
       "        (theta): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate the model structure\n",
    "text_model = MyHGNN(in_dim, hidden_dim, out_dim)\n",
    "\n",
    "# Load the saved state_dict\n",
    "text_model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "text_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'hg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     text_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnigga i hate stupid accusations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get feature embedding or logits\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     image_embedding \u001b[38;5;241m=\u001b[39m img_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/admin/Documents/latefusion/test_img_resized/1117701344800980995.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnamed_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, Parameter]]:\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;124;03m    name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1732\u001b[0m \n\u001b[1;32m   1733\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1734\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_named_members(\n\u001b[1;32m   1735\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m module: module\u001b[38;5;241m.\u001b[39m_parameters\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m-> 1736\u001b[0m         prefix\u001b[38;5;241m=\u001b[39mprefix, recurse\u001b[38;5;241m=\u001b[39mrecurse)\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m gen:\n\u001b[1;32m   1738\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m elem\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuffers\u001b[39m(\u001b[38;5;28mself\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tensor]:\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module buffers.\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m \n\u001b[1;32m   1743\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;124;03m        recurse (bool): if True, then yields buffers of this module\u001b[39;00m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;124;03m            and all submodules. Otherwise, yields only buffers that\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;124;03m            are direct members of this module.\u001b[39;00m\n\u001b[0;32m-> 1747\u001b[0m \n\u001b[1;32m   1748\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;124;03m        torch.Tensor: module buffer\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \n\u001b[1;32m   1751\u001b[0m \u001b[38;5;124;03m    Example::\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \n\u001b[1;32m   1753\u001b[0m \u001b[38;5;124;03m        >>> # xdoctest: +SKIP(\"undefined vars\")\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;124;03m        >>> for buf in model.buffers():\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;124;03m        >>>     print(type(buf), buf.size())\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;124;03m        <class 'torch.Tensor'> (20L,)\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;124;03m        <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \n\u001b[1;32m   1759\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_buffers(recurse\u001b[38;5;241m=\u001b[39mrecurse):\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m buf\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'hg'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    text_embedding = text_model(\"nigga i hate stupid accusations\")  # Get feature embedding or logits\n",
    "    image_embedding = img_model(\"/Users/admin/Documents/latefusion/test_img_resized/1117701344800980995.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
