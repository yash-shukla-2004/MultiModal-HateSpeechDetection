{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   clean_tweet_2  label\n",
      "0                                          nigga      4\n",
      "1                                horses retarded      5\n",
      "2  nigga momma youngboy spitting real shit nigga      0\n",
      "3            rt xxsugvngxx ran holy nigga today       1\n",
      "4                       everybody calling nigger      1\n",
      "Number of sentences: 149822\n",
      "Number of labels: 149822\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"textual_data_an1_cleaned_final.csv\")\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "filtered_data = data.dropna(subset=['clean_tweet_2']).copy()\n",
    "\n",
    "# Now extract both in sync\n",
    "sentences = filtered_data['clean_tweet_2'].astype(str).tolist()\n",
    "y = filtered_data['label'].astype(int).tolist()\n",
    "\n",
    "# Optional: binarize the labels\n",
    "y = [1 if label != 0 else 0 for label in y]\n",
    "\n",
    "# Confirm\n",
    "print(\"Number of sentences:\", len(sentences))\n",
    "print(\"Number of labels:\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'hate': [-2.48300359e-01  1.09273024e-01 -6.71660364e-01  2.27219760e-01\n",
      "  5.62640250e-01  4.35334861e-01 -3.13606173e-01  1.45046905e-01\n",
      "  8.83883834e-01 -9.38965857e-01 -5.20260274e-01 -6.16742909e-01\n",
      "  1.44336507e-01  9.28647995e-01  2.25819033e-02 -2.20994428e-01\n",
      "  5.68438768e-01  5.45261726e-02 -1.63975745e-01 -1.06441414e+00\n",
      "  5.85953966e-02  8.31971586e-01  5.65044768e-02 -2.93443680e-01\n",
      " -6.65605485e-01 -5.01947463e-01  5.17006874e-01 -1.88412443e-02\n",
      "  1.31390961e-02  5.62422037e-01  8.77174914e-01  3.13685179e-01\n",
      "  2.81409472e-01 -2.45108113e-01  8.27184618e-02  1.84413284e-01\n",
      " -2.70475745e-01  7.68536210e-01 -3.24853063e-01 -6.84299786e-03\n",
      " -7.70946443e-01  4.64807063e-01  9.36575606e-02 -5.06642580e-01\n",
      " -5.20099342e-01 -3.12623739e-01 -5.40407717e-01  5.22264242e-01\n",
      "  2.42474422e-01 -1.88420102e-01  2.39965487e-02 -2.40830854e-01\n",
      " -3.88183519e-02  1.61293417e-03  5.61385788e-02 -2.33369589e-01\n",
      " -1.17531925e-01 -3.83623280e-02  3.10898662e-01  1.84851035e-01\n",
      "  3.99298370e-01 -8.20589066e-03  1.98454604e-01  3.75899613e-01\n",
      "  5.45479991e-02  7.06236660e-01 -2.11034585e-02  3.96103948e-01\n",
      " -4.57616597e-02  1.15646183e+00  6.25710934e-02  2.63853192e-01\n",
      "  4.19382453e-01 -4.35088158e-01  5.87080829e-02 -1.17196470e-01\n",
      "  6.55771673e-01 -3.21597844e-01  1.08841874e-01  3.33850570e-02\n",
      " -6.80096030e-01 -7.54253864e-02 -5.26484191e-01  4.48120892e-01\n",
      "  1.56172574e-01 -5.71991384e-01 -1.10734046e+00 -6.18310121e-04\n",
      "  3.83658893e-02  1.39674485e-01 -8.90088618e-01  2.28860155e-01\n",
      " -7.15296715e-02  2.01859847e-01  1.10852532e-01  4.32395667e-01\n",
      "  4.80758458e-01 -7.98500851e-02  1.84663281e-01 -8.59404281e-02]\n",
      "Similar words: [('haterz', 0.8705105781555176), ('yhate', 0.8700005412101746), ('hatee', 0.8631404042243958), ('ihate', 0.8565041422843933), ('haten', 0.8548374176025391), ('hateonme', 0.8413458466529846), ('luv2hate', 0.8390507698059082), ('hatejust', 0.8156735301017761), ('hatersgonnahate', 0.8136341571807861), ('haterswillhate', 0.8092218041419983)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "model = FastText(vector_size=100, window=5, min_count=1, sg=1, epochs=10)\n",
    "model = FastText.load(\"fasttext_model.model\")\n",
    "\n",
    "word_vector = model.wv['hate']\n",
    "print(\"Vector for 'hate':\", word_vector)\n",
    "\n",
    "similar_words = model.wv.most_similar('hate')\n",
    "print(\"Similar words:\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (60373, 100)\n",
      "Shape of H: (60373, 149822)\n",
      "Feature Matrix Shape (X): (60373, 100)\n",
      "Incidence Matrix Shape (H): (60373, 149822)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load FastText model\n",
    "model = FastText.load(\"fasttext_model.model\")\n",
    "\n",
    "# Sample sentences\n",
    "sentences = data['clean_tweet_2']\n",
    "# Ensure all sentences are strings and handle NaN values\n",
    "sentences = [str(sentence) for sentence in sentences if pd.notnull(sentence)]\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = list(model.wv.index_to_key)\n",
    "# Create a word-to-index dictionary for faster lookups\n",
    "word_to_index = defaultdict(lambda: -1, {word: i for i, word in enumerate(vocab)})\n",
    "\n",
    "# Create embedding matrix (X) - word embeddings\n",
    "embedding_dim = model.vector_size\n",
    "X = np.zeros((len(vocab), embedding_dim), dtype=np.float32)\n",
    "print(\"Shape of X:\", X.shape)\n",
    "for i, word in enumerate(vocab):\n",
    "    X[i] = model.wv[word]\n",
    "\n",
    "# Create hypergraph incidence matrix (H)\n",
    "num_sentences = len(sentences)\n",
    "H = np.zeros((len(vocab), num_sentences), dtype=np.float32)\n",
    "\n",
    "print(\"Shape of H:\", H.shape)\n",
    "\n",
    "for j, sentence in enumerate(sentences):\n",
    "    for word in sentence.split():\n",
    "        i = word_to_index[word]\n",
    "        if i != -1:\n",
    "            H[i, j] = 1\n",
    "\n",
    "# Convert to sparse matrices\n",
    "X_sparse = csr_matrix(X)  # Feature matrix\n",
    "H_sparse = csr_matrix(H)  # Hypergraph incidence matrix\n",
    "\n",
    "print(\"Feature Matrix Shape (X):\", X_sparse.shape)\n",
    "print(\"Incidence Matrix Shape (H):\", H_sparse.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperedges (tweets): 149822\n",
      "Feature matrix X shape: (60373, 100)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import dhg\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vocab = list(model.wv.index_to_key)\n",
    "\n",
    "from collections import defaultdict\n",
    "word_to_index = defaultdict(lambda: -1, {word: idx for idx, word in enumerate(vocab)})\n",
    "\n",
    "hyperedges = []\n",
    "for sentence in sentences:\n",
    "    word_indices = list({word_to_index[word] for word in sentence.split() if word_to_index[word] != -1})\n",
    "    hyperedges.append(word_indices)\n",
    "\n",
    "print(f\"Number of hyperedges (tweets): {len(hyperedges)}\")\n",
    "\n",
    "G = dhg.Hypergraph(num_v=len(vocab), e_list=hyperedges)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "\n",
    "X = np.zeros((len(vocab), embedding_dim), dtype=np.float32)\n",
    "\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    X[i] = model.wv[word]\n",
    "\n",
    "print(\"Feature matrix X shape:\", X.shape)\n",
    "\n",
    "num_edges = len(hyperedges)  # each tweet is a hyperedge\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(num_edges),\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "test_idx = torch.tensor(test_idx, dtype=torch.long)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float)\n",
    "print(X_tensor.shape[1])\n",
    "from dhg import Hypergraph\n",
    "hg = Hypergraph(len(vocab), hyperedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of hyperedges: <class 'list'>\n",
      "Length of hyperedges: 149822\n",
      "tensor([[ 2.4595e+00, -1.1357e+00],\n",
      "        [ 1.0687e+00, -4.8374e-01],\n",
      "        [ 9.1407e-01, -5.1038e-01],\n",
      "        ...,\n",
      "        [ 1.2063e-02,  1.8870e-03],\n",
      "        [ 9.1603e-03, -1.3453e-03],\n",
      "        [ 1.1523e-02,  4.4529e-04]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dhg.models import HGNN\n",
    "\n",
    "class MyHGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.hgnn = HGNN(in_dim, hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, hg):\n",
    "        return self.hgnn(x, hg)\n",
    "\n",
    "in_dim = X_tensor.shape[1]     # embedding dimension (100 for FastText)\n",
    "hidden_dim = 64\n",
    "out_dim = 2                    # binary classification: hate or not\n",
    "\n",
    "# Recreate the model structure\n",
    "text_model = MyHGNN(in_dim, hidden_dim, out_dim)\n",
    "\n",
    "# Load the saved state_dict\n",
    "text_model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "text_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "if isinstance(y, list):\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "print(\"Type of hyperedges:\", type(hyperedges))\n",
    "print(\"Length of hyperedges:\", len(hyperedges))\n",
    "\n",
    "def predict_sentence(sentence, text_model, ft_model, vocab, X_tensor):\n",
    "    text_model.eval()\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    node_indices = [vocab[word] for word in words if word in vocab]\n",
    "\n",
    "    if not node_indices:\n",
    "        return \"No known words in sentence.\"\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = text_model(X_tensor, hg)  # shape: (num_nodes, out_dim)\n",
    "        sentence_embed = out[node_indices].mean(dim=0)  # average over node predictions\n",
    "        prediction = torch.argmax(sentence_embed).item()\n",
    "        probs = torch.softmax(sentence_embed, dim=0)\n",
    "\n",
    "    label = \"Hate Speech\" if prediction == 1 else \"Not Hate Speech\"\n",
    "    confidence = probs[prediction].item()\n",
    "\n",
    "    #return f\"Prediction: {label} (confidence: {confidence:.4f})\"\n",
    "    return out\n",
    "\n",
    "ft_model = FastText.load(\"fasttext_model.model\")\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "result = predict_sentence(\"nigga i hate tests\", text_model, ft_model, vocab_dict, X_tensor)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 149823\n",
      "Test set size: 10000\n",
      "Original label distribution (majority vote):\n",
      "NotHate (0): 124003 (82.77%)\n",
      "Racist (1): 12288 (8.20%)\n",
      "Sexist (2): 3671 (2.45%)\n",
      "Homophobe (3): 3886 (2.59%)\n",
      "Religion (4): 164 (0.11%)\n",
      "OtherHate (5): 5811 (3.88%)\n",
      "Binary label distribution:\n",
      "NotHate (0): 112858 (75.33%)\n",
      "Hate (1): 36965 (24.67%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Define paths\n",
    "DATA_ROOT = ''\n",
    "IMG_DIR = \"/Users/admin/Documents/latefusion/test_img_resized\"\n",
    "GT_FILE = os.path.join(DATA_ROOT, 'MMHS150K_GT.json')\n",
    "TRAIN_IDS = os.path.join(DATA_ROOT, 'splits', 'train_ids.txt')\n",
    "#VAL_IDS = os.path.join(DATA_ROOT, 'splits', 'val_ids.txt')\n",
    "TEST_IDS = \"/Users/admin/Documents/latefusion/test_ids.txt\"\n",
    "\n",
    "import json\n",
    "def load_ground_truth():\n",
    "    \"\"\"Load the ground truth JSON file with metadata and labels.\"\"\"\n",
    "    with open(GT_FILE, 'r') as f:\n",
    "        gt_data = json.load(f)\n",
    "    return gt_data\n",
    "\n",
    "def load_ids_from_file(file_path):\n",
    "    \"\"\"Load image IDs from a file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load ground truth data\n",
    "gt_data = load_ground_truth()\n",
    "print(f\"Total number of samples: {len(gt_data)}\")\n",
    "\n",
    "test_ids = load_ids_from_file(TEST_IDS)\n",
    "\n",
    "#print(f\"Train set size: {len(train_ids)}\")\n",
    "#print(f\"Validation set size: {len(val_ids)}\")\n",
    "print(f\"Test set size: {len(test_ids)}\")\n",
    "label_distribution = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
    "for metadata in gt_data.values():\n",
    "    # Get majority vote for each sample\n",
    "    labels = metadata['labels']\n",
    "    majority_label = max(set(labels), key=labels.count)\n",
    "    label_distribution[majority_label] += 1\n",
    "\n",
    "print(\"Original label distribution (majority vote):\")\n",
    "label_names = [\"NotHate\", \"Racist\", \"Sexist\", \"Homophobe\", \"Religion\", \"OtherHate\"]\n",
    "for label_idx, count in label_distribution.items():\n",
    "    percentage = count / len(gt_data) * 100\n",
    "    print(f\"{label_names[label_idx]} ({label_idx}): {count} ({percentage:.2f}%)\")\n",
    "\n",
    "def convert_to_binary(labels):\n",
    "    \"\"\"\n",
    "    Convert the 6-class labels to binary labels.\n",
    "    \n",
    "    Args:\n",
    "        labels: List of original labels (0-5)\n",
    "        \n",
    "    Returns:\n",
    "        Binary label (0 for NotHate, 1 for Hate)\n",
    "    \"\"\"\n",
    "    # For each annotator's label, convert to binary (0 remains 0, 1-5 become 1)\n",
    "    binary_labels = [1 if label > 0 else 0 for label in labels]\n",
    "    \n",
    "    # Take majority vote for the final label\n",
    "    return round(sum(binary_labels) / len(binary_labels))\n",
    "\n",
    "# Process all data and create a binary label mapping\n",
    "binary_labels = {}\n",
    "for img_id, metadata in gt_data.items():\n",
    "    binary_labels[img_id] = convert_to_binary(metadata['labels'])\n",
    "\n",
    "# Count the distribution of binary labels\n",
    "label_counts = {0: 0, 1: 0}\n",
    "for label in binary_labels.values():\n",
    "    label_counts[label] += 1\n",
    "\n",
    "print(\"Binary label distribution:\")\n",
    "print(f\"NotHate (0): {label_counts[0]} ({label_counts[0]/len(binary_labels)*100:.2f}%)\")\n",
    "print(f\"Hate (1): {label_counts[1]} ({label_counts[1]/len(binary_labels)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary labels saved to binary_labels.json\n",
      "Selected 10000 images for training.\n",
      "Found 1000 valid images out of 1000 IDs\n"
     ]
    }
   ],
   "source": [
    "# Save binary labels to a file\n",
    "with open('binary_labels.json', 'w') as f:\n",
    "    json.dump(binary_labels, f)\n",
    "\n",
    "print(\"Binary labels saved to binary_labels.json\")\n",
    "\n",
    "def load_binary_labels(json_path='binary_labels.json'):\n",
    "    \"\"\"Load binary labels from a JSON file.\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        binary_labels = json.load(f)\n",
    "    return binary_labels\n",
    "\n",
    "from tensorflow.keras.utils import Sequence  # <-- Add this line\n",
    "\n",
    "class MMHS150KSequence(Sequence):\n",
    "    \"\"\"Custom data generator for the MMHS150K dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, binary_labels, img_ids, batch_size=32, \n",
    "                 img_size=(224, 224), shuffle=True):\n",
    "        \"\"\"\n",
    "        Initialize the data generator.\n",
    "        \n",
    "        Args:\n",
    "            img_dir (str): Directory with all the images\n",
    "            binary_labels (dict): Dictionary mapping image IDs to binary labels\n",
    "            img_ids (list): List of image IDs to include in this dataset\n",
    "            batch_size (int): Batch size\n",
    "            img_size (tuple): Target image size (height, width)\n",
    "            shuffle (bool): Whether to shuffle the data after each epoch\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.binary_labels = binary_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Filter to include only images that exist\n",
    "        self.valid_img_ids = []\n",
    "        for img_id in img_ids:\n",
    "            if img_id in binary_labels and os.path.exists(os.path.join(img_dir, f\"{img_id}.jpg\")):\n",
    "                self.valid_img_ids.append(img_id)\n",
    "        \n",
    "        print(f\"Found {len(self.valid_img_ids)} valid images out of {len(img_ids)} IDs\")\n",
    "        \n",
    "        # Create indices and shuffle if needed\n",
    "        self.indices = np.arange(len(self.valid_img_ids))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches per epoch.\"\"\"\n",
    "        return int(np.ceil(len(self.valid_img_ids) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get batch at index idx.\"\"\"\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        # Initialize batch arrays\n",
    "        batch_x = np.zeros((len(batch_indices), self.img_size[0], self.img_size[1], 3), dtype=np.float32)\n",
    "        batch_y = np.zeros(len(batch_indices), dtype=np.int32)\n",
    "        batch_ids = []\n",
    "        \n",
    "        # Load and preprocess images\n",
    "        for i, idx in enumerate(batch_indices):\n",
    "            img_id = self.valid_img_ids[idx]\n",
    "            img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n",
    "            \n",
    "            try:\n",
    "                # Load and preprocess image\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img = img.resize(self.img_size)\n",
    "                img_array = img_to_array(img)\n",
    "                \n",
    "                # Preprocess for the appropriate model\n",
    "                # For ResNet: normalize with the ImageNet statistics\n",
    "                img_array = img_array / 255.0\n",
    "                img_array = (img_array - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "                \n",
    "                batch_x[i] = img_array\n",
    "                batch_y[i] = self.binary_labels[img_id]\n",
    "                batch_ids.append(img_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                # Use zeros for failed images\n",
    "                batch_x[i] = np.zeros((self.img_size[0], self.img_size[1], 3))\n",
    "                batch_y[i] = self.binary_labels[img_id]  # Use label anyway\n",
    "                batch_ids.append(img_id)\n",
    "        \n",
    "        return batch_x, batch_y, batch_ids\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Called at the end of each epoch.\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def get_sample(self, idx):\n",
    "        \"\"\"Get a single sample for visualization.\"\"\"\n",
    "        img_id = self.valid_img_ids[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = img.resize(self.img_size)\n",
    "            img_array = img_to_array(img)\n",
    "            \n",
    "            # Normalize the image (for visualization)\n",
    "            img_array = img_array / 255.0\n",
    "            \n",
    "            # Get label\n",
    "            label = self.binary_labels[img_id]\n",
    "            \n",
    "            return img_array, label, img_id\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return np.zeros((self.img_size[0], self.img_size[1], 3)), self.binary_labels[img_id], img_id\n",
    "        \n",
    "train_ids = sorted([f.split('.')[0] for f in os.listdir(IMG_DIR) \n",
    "                    if f.lower().endswith(('.jpg', '.png', '.jpeg')) and f.split('.')[0] in test_ids])\n",
    "\n",
    "print(f\"Selected {len(train_ids)} images for training.\")\n",
    "\n",
    "import numpy as np\n",
    "# Create a sample data generator with a subset of train images\n",
    "sample_train_ids = train_ids[:1000]  # Use a small subset for faster processing\n",
    "sample_generator = MMHS150KSequence(IMG_DIR, binary_labels, sample_train_ids, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using resnet50 as feature extractor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 32/32 [11:37<00:00, 21.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (1000, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50, ResNet101, EfficientNetB0, VGG16, MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "\n",
    "def get_feature_extractor(model_name='resnet50', img_size=(224, 224)):\n",
    "    models_dict = {\n",
    "        'resnet50': ResNet50,\n",
    "        'resnet101': ResNet101,\n",
    "        'efficientnet_b0': EfficientNetB0,\n",
    "        'vgg16': VGG16,\n",
    "        'mobilenet_v2': MobileNetV2\n",
    "    }\n",
    "    \n",
    "    if model_name not in models_dict:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}. Choose from {list(models_dict.keys())}\")\n",
    "\n",
    "    base_model = models_dict[model_name](\n",
    "        weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3)\n",
    "    )\n",
    "    \n",
    "    base_model.trainable = False  # Freeze model weights\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    feature_extractor = Model(inputs=base_model.input, outputs=x, name=f\"{model_name}_feature_extractor\")\n",
    "\n",
    "    return feature_extractor\n",
    "\n",
    "def extract_features(data_generator, feature_extractor):\n",
    "    features = []\n",
    "    labels = []\n",
    "    img_ids = []\n",
    "    \n",
    "    for i in tqdm(range(len(data_generator)), desc=\"Extracting features\"):\n",
    "        batch_imgs, batch_labels, batch_ids = data_generator[i]\n",
    "        batch_features = feature_extractor.predict(batch_imgs, verbose=0)\n",
    "\n",
    "        features.append(batch_features)\n",
    "        labels.extend(batch_labels)\n",
    "        img_ids.extend(batch_ids)\n",
    "    \n",
    "    features = np.vstack(features)  # Stack features into a single array\n",
    "\n",
    "    return features, np.array(labels), img_ids\n",
    "\n",
    "# Example usage\n",
    "model_name = 'resnet50'\n",
    "img_size = (224, 224)\n",
    "feature_extractor = get_feature_extractor(model_name=model_name, img_size=img_size)\n",
    "print(f\"Using {model_name} as feature extractor\")\n",
    "\n",
    "# Assuming `sample_generator` is properly defined\n",
    "sample_features, sample_labels, sample_img_ids = extract_features(sample_generator, feature_extractor)\n",
    "print(f\"Extracted features shape: {sample_features.shape}\")\n",
    "import torch\n",
    "\n",
    "# Convert extracted features to PyTorch tensor\n",
    "sample_features_torch = torch.tensor(sample_features, dtype=torch.float32)\n",
    "sample_labels_torch = torch.tensor(sample_labels, dtype=torch.long)  # If labels are integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypergraph Incidence Matrix Shape: torch.Size([1000, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HypergraphNeuralNetwork(\n",
       "  (hgconv1): HypergraphConv(2048, 1024)\n",
       "  (hgconv2): HypergraphConv(1024, 2)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "def build_knn_hypergraph(features, k=5):\n",
    "    \"\"\"\n",
    "    Construct hyperedges using k-Nearest Neighbors (k-NN).\n",
    "    \n",
    "    Args:\n",
    "        features (torch.Tensor): Tensor of shape (N, D) where N is the number of images, D is the feature dimension.\n",
    "        k (int): Number of neighbors to connect per node.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Hypergraph incidence matrix H of shape (N, E), where E is the number of hyperedges.\n",
    "    \"\"\"\n",
    "    N = features.shape[0]\n",
    "    features_np = features.cpu().numpy()  # Convert to NumPy for k-NN computation\n",
    "    \n",
    "    # Compute k-NN graph\n",
    "    knn = NearestNeighbors(n_neighbors=k + 1, metric='cosine').fit(features_np)\n",
    "    neighbors = knn.kneighbors(features_np, return_distance=False)\n",
    "    \n",
    "    # Create hyperedges (excluding self-loops)\n",
    "    hyperedges = []\n",
    "    for i in range(N):\n",
    "        hyperedges.append(set(neighbors[i][1:]))  # Exclude self (first entry is the node itself)\n",
    "    \n",
    "    # Convert to hypergraph incidence matrix (N x E)\n",
    "    num_hyperedges = len(hyperedges)\n",
    "    H = torch.zeros((N, num_hyperedges), dtype=torch.float32)\n",
    "    \n",
    "    for e, nodes in enumerate(hyperedges):\n",
    "        for node in nodes:\n",
    "            H[node, e] = 1.0  # Assign weight 1 to hyperedges\n",
    "    \n",
    "    return H\n",
    "\n",
    "# Example usage:\n",
    "k = 5  # Number of neighbors per node\n",
    "H = build_knn_hypergraph(sample_features_torch, k)\n",
    "print(f\"Hypergraph Incidence Matrix Shape: {H.shape}\")  # Should be (N, E)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "from torch_geometric.data import Data\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class HypergraphNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.5):\n",
    "        super(HypergraphNeuralNetwork, self).__init__()\n",
    "\n",
    "        # Hypergraph convolution layers\n",
    "        self.hgconv1 = HypergraphConv(in_dim, hidden_dim)\n",
    "        self.hgconv2 = HypergraphConv(hidden_dim, out_dim)\n",
    "\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.hgconv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.hgconv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Convert hypergraph incidence matrix (H) to edge_index format for PyTorch Geometric\n",
    "edge_index = H.nonzero().t().contiguous()\n",
    "\n",
    "# Define HGNN model\n",
    "in_dim = sample_features_torch.shape[1]  # 2048 (ResNet50 features)\n",
    "hidden_dim = 1024  # Increased hidden dimension\n",
    "out_dim = len(torch.unique(sample_labels_torch))  # Number of classes\n",
    "img_model = HypergraphNeuralNetwork(in_dim, hidden_dim, out_dim)\n",
    "\n",
    "# Load the saved state_dict\n",
    "img_model.load_state_dict(torch.load(\"model_weights2.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "img_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Test Image Embedding Shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def extract_single_image_feature(img_path, feature_extractor):\n",
    "    \"\"\"\n",
    "    Extract feature vector for a single test image using the CNN feature extractor.\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")  # Load and convert to RGB\n",
    "    img = img.resize((224, 224))  # Resize to match model input size\n",
    "    \n",
    "    # Convert image to array and preprocess\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = preprocess_input(img_array)  # Normalize as per ResNet50\n",
    "    \n",
    "    # Extract feature vector\n",
    "    feature_vector = feature_extractor.predict(img_array)\n",
    "    return torch.tensor(feature_vector, dtype=torch.float32)  # Convert to torch tensor\n",
    "\n",
    "# Example usage\n",
    "test_img_path = \"/Users/admin/Documents/latefusion/test_img_resized/1117680664667729922.jpg\"\n",
    "test_feature = extract_single_image_feature(test_img_path, feature_extractor)  # Extract features\n",
    "\n",
    "# Add the test image to the existing dataset\n",
    "all_features = torch.cat([sample_features_torch, test_feature], dim=0)\n",
    "\n",
    "# Recompute the hypergraph (including the test image)\n",
    "H_new = build_knn_hypergraph(all_features, k=5)\n",
    "\n",
    "# Convert the new hypergraph into edge_index format\n",
    "edge_index_new = H_new.nonzero().t().contiguous()\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "img_model.eval()\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = img_model(all_features, edge_index_new)\n",
    "\n",
    "# Extract the last embedding (corresponding to the test image)\n",
    "test_embedding = embeddings[-1].detach().cpu().numpy()\n",
    "\n",
    "print(\"Test Image Embedding Shape:\", test_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8544, 0.1456],\n",
      "        [0.6787, 0.3213],\n",
      "        [0.6646, 0.3354],\n",
      "        ...,\n",
      "        [0.4942, 0.5058],\n",
      "        [0.4942, 0.5058],\n",
      "        [0.4943, 0.5057]])\n"
     ]
    }
   ],
   "source": [
    "text_probs = result\n",
    "image_probs = test_embedding\n",
    "avg_probs = (text_probs + image_probs) / 2\n",
    "avg_probs = torch.softmax(avg_probs, dim=1)\n",
    "\n",
    "print(avg_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8544, 0.1456],\n",
      "        [0.6787, 0.3213],\n",
      "        [0.6646, 0.3354],\n",
      "        ...,\n",
      "        [0.4942, 0.5058],\n",
      "        [0.4942, 0.5058],\n",
      "        [0.4943, 0.5057]])\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.6  # Adjust weights as needed\n",
    "wavg_probs = alpha * text_probs + (1 - alpha) * image_probs\n",
    "wavg_probs = torch.softmax(wavg_probs, dim=1)\n",
    "\n",
    "print(avg_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text probs shape: torch.Size([2, 2])\n",
      "Image probs shape: torch.Size([2])\n",
      "Fused embedding shape: torch.Size([2, 3])\n",
      "Accuracy: 0.0\n",
      "Final Output (logits): tensor([[ 0.0237, -0.2506],\n",
      "        [ 0.0699, -0.1191]], grad_fn=<AddmmBackward0>)\n",
      "Final Probabilities: tensor([[0.5682, 0.4318],\n",
      "        [0.5471, 0.4529]], grad_fn=<SoftmaxBackward0>)\n",
      "Predicted Classes: tensor([0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/6x2ghpdd4dzc3df6gt3zdn2c0000gn/T/ipykernel_4741/2110401324.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_true = torch.tensor(y[:min_batch]).to(device)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np  # Ensure NumPy is imported\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors (if needed)\n",
    "text_probs = torch.tensor(text_probs) if isinstance(text_probs, np.ndarray) else text_probs\n",
    "image_probs = torch.tensor(image_probs) if isinstance(image_probs, np.ndarray) else image_probs\n",
    "\n",
    "# Ensure both are on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "text_probs = text_probs.to(device)\n",
    "image_probs = image_probs.to(device)\n",
    "# Print to debug shape mismatch\n",
    "print(\"Text probs shape:\", text_probs.shape)\n",
    "print(\"Image probs shape:\", image_probs.shape)\n",
    "\n",
    "# Ensure both tensors have the same batch size\n",
    "min_batch = min(text_probs.shape[0], image_probs.shape[0])\n",
    "text_probs = text_probs[:min_batch]  \n",
    "image_probs = image_probs[:min_batch]  \n",
    "# Ensure both tensors are 2D\n",
    "if text_probs.dim() == 1:\n",
    "    text_probs = text_probs.unsqueeze(1)  # (N,) -> (N,1)\n",
    "if image_probs.dim() == 1:\n",
    "    image_probs = image_probs.unsqueeze(1)  # (N,) -> (N,1)\n",
    "\n",
    "# Concatenate the tensors\n",
    "fused_embedding = torch.cat((text_probs, image_probs), dim=1)\n",
    "\n",
    "# Check the final shape\n",
    "print(\"Fused embedding shape:\", fused_embedding.shape)\n",
    "\n",
    "# Ensure both tensors have the same number of dimensions\n",
    "if text_probs.dim() == 1:  \n",
    "    text_probs = text_probs.unsqueeze(1)  # Convert (N,) -> (N, 1)\n",
    "\n",
    "if image_probs.dim() == 1:\n",
    "    image_probs = image_probs.unsqueeze(1)  # Convert (N,) -> (N, 1)\n",
    "\n",
    "# Concatenate the tensors\n",
    "fused_embedding = torch.cat((text_probs, image_probs), dim=1)\n",
    "\n",
    "# Define the fusion layer (MLP)\n",
    "fusion_layer = nn.Sequential(\n",
    "    nn.Linear(fused_embedding.shape[1], 128),  # Hidden layer\n",
    "    nn.ReLU(),  # Non-linearity\n",
    "    nn.Linear(128, 2)  # Output layer (adjust based on number of classes)\n",
    ").to(device)  # Move model to correct device\n",
    "\n",
    "# Forward pass through MLP\n",
    "final_output = fusion_layer(fused_embedding)\n",
    "\n",
    "# Convert to probabilities if required\n",
    "final_probs = F.softmax(final_output, dim=1)\n",
    "\n",
    "# Get final class predictions\n",
    "final_class = torch.argmax(final_probs, dim=1)\n",
    "# Assuming `y` contains your true labels\n",
    "y_true = torch.tensor(y[:min_batch]).to(device)\n",
    "\n",
    "# Compute accuracy\n",
    "correct = (final_class == y_true).sum().item()\n",
    "total = y_true.size(0)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "print(\"Final Output (logits):\", final_output)\n",
    "print(\"Final Probabilities:\", final_probs)\n",
    "print(\"Predicted Classes:\", final_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8158861398696899\n",
      "Epoch 2, Loss: 0.7695637345314026\n",
      "Epoch 3, Loss: 0.7252331972122192\n",
      "Epoch 4, Loss: 0.6828690767288208\n",
      "Epoch 5, Loss: 0.6425489187240601\n",
      "Epoch 6, Loss: 0.6042916178703308\n",
      "Epoch 7, Loss: 0.5680785179138184\n",
      "Epoch 8, Loss: 0.5338338613510132\n",
      "Epoch 9, Loss: 0.5015227198600769\n",
      "Epoch 10, Loss: 0.47110018134117126\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fusion_layer.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (example for 10 epochs)\n",
    "for epoch in range(10):\n",
    "    fusion_layer.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = fusion_layer(fused_embedding)\n",
    "    loss = criterion(outputs, y_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy\n",
    "correct = (final_class == y_true).sum().item()\n",
    "total = y_true.size(0)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 2})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(y_true.cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text probs shape: torch.Size([2, 2])\n",
      "Image probs shape: torch.Size([2, 1])\n",
      "y_true shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Text probs shape:\", text_probs.shape)\n",
    "print(\"Image probs shape:\", image_probs.shape)\n",
    "print(\"y_true shape:\", y_true.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
